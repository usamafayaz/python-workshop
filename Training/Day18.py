# Recurrent NN is memory based, and it also preserves time.

# Attention. (Attention is all you need!) published in 2014.
# Attention is the important region of the input which is required for certain job.
# Example in Text Domain. => Cyber Bullying Detection.
# Example in Sentimental Analysis. => only on adjectives, or maybe nouns.
# Example in Image Processing. => to

# https://scholar.google.com/ for all the Research Paper.
# ICML (International Conference on Machine Learning)

# Sequence to Sequence with RNNs- Translation.
# Two conditions:
# 1- Context Extraction
# 2- Good Training of Decoder

# Details will be missed in case of long input size because of single contexct cell.
# Padding Size.

# Reference Material.
# Prof. Justin Johnson (Stanford)
# Deep Learning MIT
# Krish Naik
# DeepLearning using Python Francis Chollet.